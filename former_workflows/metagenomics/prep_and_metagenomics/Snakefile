################################################################################################################
####################################       PREPROCESSING+METAGENOMICS      #####################################
################################################################################################################

# configfile specified in command line

##
# Quality-filtering
##

rule qual_filt:
    input:
        read1="{projectpath}/00-InputData/{sample}_1.fastq.gz",
        read2="{projectpath}/00-InputData/{sample}_2.fastq.gz"
    output:
        read1="{projectpath}/01-QualityFiltered/{sample}_1.fastq",
        read2="{projectpath}/01-QualityFiltered/{sample}_2.fastq",
        stats_file="{projectpath}/01-QualityFiltered/{sample}.stats"
    params:
        adapter1=expand("{adapter1}", adapter1=config['adapter1']),
        adapter2=expand("{adapter2}", adapter2=config['adapter2']),
        maxns=expand("{maxns}", maxns=config['maxns']),
        minquality=expand("{minquality}", minquality=config['minquality']),
        threads=expand("{threads}", threads=config['threads'])
    run:
        import time
        import gzip
        statsfile=open(output.stats_file,"w+")
        current_time = time.strftime("%m.%d.%y %H:%M", time.localtime())
        statsfile.write("Statistic\tValue \r\n".format(current_time))

        #Get initial stats
        reads = 0
        bases = 0
        #If gzipped
        import os
        if str(input.read1).endswith('.gz'):
            with gzip.open(str(input.read1), 'rb') as read:
                for id in read:
                    seq = next(read)
                    reads += 1
                    bases += len(seq.strip())*2
                    next(read)
                    next(read)
        else:
            with open(input.read1, 'rb') as read:
                for id in read:
                    seq = next(read)
                    reads += 1
                    bases += len(seq.strip())*2
                    next(read)
                    next(read)
        statsfile.write("Input reads\t{0} ({1} bases)\r\n".format(reads,bases))
        statsfile.close()


        shell("module unload gcc tools ngs && module load tools gcc/5.4.0 AdapterRemoval/2.1.3 && AdapterRemoval --file1 {input.read1} --file2 {input.read2} --output1 {output.read1} --output2 {output.read2} --trimqualities --trimns --maxns {params.maxns} --minquality {params.minquality} --threads {params.threads} --adapter1 {params.adapter1} --adapter2 {params.adapter2}")

        #Get stats after quality filtering
        reads = 0
        bases = 0
        with open(str(output.read1), 'rb') as read:
            for id in read:
                seq = next(read)
                reads += 1
                bases += len(seq.strip())
                next(read)
                next(read)

        #Print stats to stats file
        statsfile=open(str(output.stats_file),"a+")
        statsfile.write("Quality filtered reads\t{0} ({1} bases)\r\n".format(reads,bases))
        statsfile.close()

##
# Duplicate removal (single-based)
##

#rule dup_rem_single:
#    input:
#      read1="{projectpath}/01-QualityFiltered/{sample}_1.fastq",
#      read2="{projectpath}/01-QualityFiltered/{sample}_2.fastq"
#    output:
#      read1="{projectpath}/02-DuplicatesRemoved/{sample}_1.fastq.tmp",
#      read2="{projectpath}/02-DuplicatesRemoved/{sample}_2.fastq.tmp"
#    run:
#      shell("module load tools pigz/2.3.4 seqkit/0.7.1 && cat {input.read1} | seqkit rmdup -s -o {output.read1}")
#      shell("module load tools pigz/2.3.4 seqkit/0.7.1 && cat {input.read2} | seqkit rmdup -s -o {output.read2}")
#
#rule dup_rem_single_repair:
#    input:
#      read1="{projectpath}/02-DuplicatesRemoved/{sample}_1.fastq.tmp",
#      read2="{projectpath}/02-DuplicatesRemoved/{sample}_2.fastq.tmp"
#    output:
#      read1="{projectpath}/02-DuplicatesRemoved/{sample}_1.fastq",
#      read2="{projectpath}/02-DuplicatesRemoved/{sample}_2.fastq"
#    shell:
#      "module load tools jre/1.8.0 bbmap/36.49 && repair.sh in={input.read1} in2={input.read2} out={output.read1} out2={output.read2} overwrite=t && rm {input.read1} {input.read2}"

##
# Duplicate removal (pair-based)
##

rule dup_rem_paired:
    input:
      read1="{projectpath}/01-QualityFiltered/{sample}_1.fastq",
      read2="{projectpath}/01-QualityFiltered/{sample}_2.fastq"
    output:
      dir="{projectpath}/02-DuplicatesRemoved/{sample}.merged.fastq",
    params:
        separator=expand("{separator}", separator=config['separator'])
    shell:
      "module load tools pigz/2.3.4 seqkit/0.7.1 && paste -d {params.separator} {input.read1} {input.read2} | seqkit rmdup -s -j 28 -o {output.dir} "



rule dup_rem_paired_repair:
    input:
      in_file="{projectpath}/02-DuplicatesRemoved/{sample}.merged.fastq",
      in_stats="{projectpath}/01-QualityFiltered/{sample}.stats"
    output:
      read1="{projectpath}/02-DuplicatesRemoved/{sample}_1.fastq",
      read2="{projectpath}/02-DuplicatesRemoved/{sample}_2.fastq",
      stats_file="{projectpath}/02-DuplicatesRemoved/{sample}.stats"
    params:
        separator=expand("{separator}", separator=config['separator'])
    run:
      shell("cut --delimiter={params.separator} -f1 {input.in_file} > {output.read1}")
      shell("cut --delimiter={params.separator} -f2 {input.in_file} > {output.read2}")
      shell("rm {input.in_file}")
      shell("mv {input.in_stats} {output.stats_file}")

      #Get stats after duplicate removal
      reads = 0
      bases = 0
      with open(str(output.read1), 'rb') as read:
        for id in read:
            seq = next(read)
            reads += 1
            bases += len(seq.strip())*2
            next(read)
            next(read)

        #Print stats to stats file
        statsfile=open(str(output.stats_file),"a+")
        statsfile.write("Dereplicated reads\t{0} ({1} bases)\r\n".format(reads,bases))
        statsfile.close()



##
# Mapping to host
##

rule map_host:
    input:
        read1="{projectpath}/02-DuplicatesRemoved/{sample}_1.fastq",
        read2="{projectpath}/02-DuplicatesRemoved/{sample}_2.fastq",
        refgenome=expand("{refgenome}", refgenome=config['refgenomehost'])
    output:
        "{projectpath}/03-MappedToHost/{sample}_all.bam"
    run:
      shell("module load tools samtools/1.9 bwa/0.7.15 && bwa mem -t 28 -R '@RG\tID:ProjectName\tCN:AuthorName\tDS:Mappingt\tPL:Illumina1.9\tSM:Sample' {input.refgenome} {input.read1} {input.read2} | samtools view -T {input.refgenome} -b - > {output}")


rule map_host_split:
    input:
        refgenome=expand("{refgenomehost}", refgenomehost=config['refgenomehost']),
        all_bam="{projectpath}/03-MappedToHost/{sample}_all.bam"
    output:
        host="{projectpath}/03-MappedToHost/{sample}_host.bam",
        read1="{projectpath}/03-MappedToHost/{sample}_1.fastq",
        read2="{projectpath}/03-MappedToHost/{sample}_2.fastq"
    shell:
        """
        module load tools samtools/1.9 && samtools view -T {input.refgenome} -b -F12 {input.all_bam} > {output.host}
        module load tools samtools/1.9 && samtools view -T {input.refgenome} -b -f12 {input.all_bam} | samtools fastq -1 {output.read1} -2 {output.read2} -
        rm {input.all_bam}
        """

##
# Mapping to human
##
rule map_human:
    input:
        read1="{projectpath}/03-MappedToHost/{sample}_1.fastq",
        read2="{projectpath}/03-MappedToHost/{sample}_2.fastq",
        refgenome=expand("{refgenomehuman}", refgenomehuman=config['refgenomehuman'])
    output:
        "{projectpath}/04-MappedToHuman/{sample}_all.bam"
    run:
      shell("module load tools samtools/1.9 bwa/0.7.15 && bwa mem -t 28 -R '@RG\tID:ProjectName\tCN:AuthorName\tDS:Mappingt\tPL:Illumina1.9\tSM:Sample' {input.refgenome} {input.read1} {input.read2} | samtools view -T {input.refgenome} -b - > {output}")


rule map_human_split:
    input:
        refgenome=expand("{refgenomehuman}", refgenomehuman=config['refgenomehuman']),
        all_bam="{projectpath}/04-MappedToHuman/{sample}_all.bam",
        in_stats="{projectpath}/02-DuplicatesRemoved/{sample}.stats"
    output:
        read1="{projectpath}/04-MappedToHuman/{sample}_1.fastq", ## mapped
        read2="{projectpath}/04-MappedToHuman/{sample}_2.fastq", ## mapped
        stats_file="{projectpath}/04-MappedToHuman/{sample}.stats"
    run:
        shell("module load tools samtools/1.9 && samtools view -T {input.refgenome} -b -f12 {input.all_bam} | samtools fastq -1 {output.read1} -2 {output.read2} -")
        shell("rm {input.all_bam}")
        shell("mv {input.in_stats} {output.stats_file}")


        #Get stats
        reads = 0
        bases = 0
        with open(str(output.read1), 'rb') as read:
            for id in read:
                seq = next(read)
                reads += 1
                bases += len(seq.strip())*2
                next(read)
                next(read)
        #Print stats to statsfile
        statsfile=open(str(output.stats_file),"a+")
        statsfile.write("Reads after mapping to reference genome \t{0} ({1} bases)\r\n".format(reads,bases))
        statsfile.close()

print("############################ Holoflow has finished PREPROCESSING, METAGENOMICS workflow starting :) ############################")


################################################################################################################
############################################       METAGENOMICS     ############################################
################################################################################################################

##
# Assembly
##
rule assembly:
    input:
        read1="{projectpath}/04-MappedToHuman/{sample}_1.fastq",
        read2="{projectpath}/04-MappedToHuman/{sample}_2.fastq"
    output:
        dir=directory("{projectpath}/05-Assembly/{sample}")
    params:
        memory=expand("{memory}", memory=config['memory']),
        klist_megahit=expand("{klist_megahit}", klist_megahit=config['klist_megahit']),
        klist_spades=expand("{klist_spades}", klist_spades=config['klist_spades']),
        threads=expand("{threads}", threads=config['threads']),
        assembler=expand("{assembler}", assembler=config['assembler'])
    run:
        if params.assembler == "megahit":
            shell("module load tools megahit/1.1.1 && megahit -1 {input.read1} -2 {input.read2} -t {params.threads} --k-list {params.klist_megahit} -o {output.dir}")

        if params.assembler == "spades":
            shell("module unload anaconda3/4.4.0 && module load tools anaconda3/2.1.0 spades/3.13.1 perl/5.20.2 && metaspades.py -1 {input.read1} -2 {input.read2} -t {params.threads} -m {params.memory} -k {params.klist_spades} --only-assembler -o {output.dir}")


rule assembly_move:
    input:
        dir=directory("{projectpath}/05-Assembly/{sample}"),
        in_stats="{projectpath}/04-MappedToHuman/{sample}.stats"
    output:
        final_file="{projectpath}/05-Assembly/{sample}/{sample}.assembly.fa",
        stats_file="{projectpath}/05-Assembly/{sample}/{sample}.stats"
    params:
        assembler=expand("{assembler}", assembler=config['assembler'])
    run:
        if params.assembler == "megahit":
            shell("mv {input.dir}/final.contigs.fa {output.final_file}")
        else:
            shell("mv {input.dir}/scaffolds.fasta {output.final_file}")

        shell("mv {input.in_stats} {output.stats_file}")

        #Get stats after assembly
        contigs = len([1 for line in open(str(output.final_file)) if line.startswith(">")])

        #Print stats to stats file
        statsfile=open(str(output.stats_file),"a+")
        statsfile.write("Assembly contigs\t{0} \r\n".format(contigs))
        statsfile.close()


rule assembly_reformat:
    input:
        dir="{projectpath}/05-Assembly/{sample}/{sample}.assembly.fa",
        in_stats="{projectpath}/05-Assembly/{sample}/{sample}.stats"
    output:
        "{projectpath}/05-Assembly/{sample}/{sample}.fa"


    run:
        with open(str(input.dir)) as f_input, open(str(output), 'w') as f_output:
            seq = ''
            contig_n = 0

            for line in f_input:
                if line.startswith('>'):

                    if seq:
                        if len(seq) > 1000:
                            contig_n += 1
                            contig_id = (">C_"+str(contig_n))
                            seq += ('\n')

                            f_output.write(contig_id + '\n' + seq)
                            seq = ''

                        else:
                            seq = ''
                else:
                    seq += line.strip()

            if seq:
                if len(seq) > 1000:
                    contig_n += 1
                    contig_id = (">C_"+str(contig_n))
                    seq += ('\n')
                    f_output.write(contig_id + '\n' + seq)

                else:
                    pass

            #Get stats after assembly reformat
            contigs = len([1 for line in open(str(output)) if line.startswith(">")])

            #Print stats to stats file
            statsfile=open(str(input.in_stats),"a+")
            statsfile.write("Reformated assembly contigs\t{0} \r\n".format(contigs))
            statsfile.close()


##
# Index assembly
##
rule index_assembly:
    input:
        "{projectpath}/05-Assembly/{sample}/{sample}.fa"
    output: # FUTURE: ADD OPTION TO REMOVE ALL BUT FA.FAI
        samtools="{projectpath}/05-Assembly/{sample}/{sample}.fa.fai",
        bwa_bwt="{projectpath}/05-Assembly/{sample}/{sample}.fa.bwt",
        bwa_pac="{projectpath}/05-Assembly/{sample}/{sample}.fa.pac",
        bwa_ann="{projectpath}/05-Assembly/{sample}/{sample}.fa.ann",
        bwa_amb="{projectpath}/05-Assembly/{sample}/{sample}.fa.amb",
        bwa_sa="{projectpath}/05-Assembly/{sample}/{sample}.fa.sa"
    run:
        if not os.path.exists("projectpath/05-Assembly/{sample}/{sample}.fa.fai"):
            shell("module load tools samtools/1.9 && samtools faidx {input} && module load tools bwa/0.7.15 && bwa index {input}")
        else:
            pass

##
# Assembly mapping
##

rule assembly_mapping:
    input:
        assembly="{projectpath}/05-Assembly/{sample}/{sample}.fa",
        read1="{projectpath}/04-MappedToHuman/{sample}_1.fastq",
        read2="{projectpath}/04-MappedToHuman/{sample}_2.fastq"
    output:
        assemblybam="{projectpath}/06-Assembly_mapping/{sample}.mapped.bam"
    params:
        threads=expand("{threads}", threads=config['threads'])
    shell:
        """
        module load tools samtools/1.9 bwa/0.7.15 && bwa mem -t {params.threads} -R "@RG\tID:ProjectName\tCN:AuthorName\tDS:Mappingt\tPL:Illumina1.9\tSM:Sample" {input.assembly} {input.read1} {input.read2} | samtools view -T {input.assembly} -b - | samtools sort -T {input.assembly} - > {output.assemblybam}
        """

##
# Prodigal ORF prediction
##
#"Metagenomes - The simplest approach for metagenomes is to put all the sequences in one FASTA file and analyze them in Anonymous Mode."
rule protein_prediction_prodigal:
    input:
        assembly="{projectpath}/05-Assembly/{sample}/{sample}.fa"
    output:
        genetic_coords="{projectpath}/06-ProdigalPrediction/{sample}.coords.gbk",
        protein_translations="{projectpath}/06-ProdigalPrediction/{sample}.protein_translations.faa"
    shell: # Prodigal is run in "anon", Anonymous workflow
        """
        module unload gcc && module load tools prodigal/2.6.3 && prodigal -i {input.assembly} -o {output.genetic_coords} -a {output.protein_translations} -p meta
        """


##
# Create depth table
##

rule depth_table:
    input:
        assemblybam="{projectpath}/06-Assembly_mapping/{sample}.mapped.bam"
    output:
        depth_file="{projectpath}/07-Binning/{sample}.depth.txt"
    shell:
        """
        module unload gcc && module load tools perl/5.20.2 metabat/2.12.1 && jgi_summarize_bam_contig_depths --outputDepth {output.depth_file} {input.assemblybam}

        """


##
# Binning with metabat
##

rule binning_metabat:
    input:
        assembly_idx="{projectpath}/05-Assembly/{sample}/{sample}.fa",
        #assemblybam="{projectpath}/06-Assembly_mapping/{sample}.mapped.bam"
        depth_file="{projectpath}/07-Binning/{sample}.depth.txt"
    output:
        dir_mtb="{projectpath}/07-Binning/{sample}.metabat",
        #depth_file="{projectpath}/07-Binning/{sample}.depth_metabat.txt",
        bin_table_mtb="{projectpath}/07-Binning/{sample}.bins_metabat.txt",
        final_file="{projectpath}/07-Binning/{sample}.bins_metabat.tar.gz"
    params:
        threads=expand("{threads}", threads=config['threads'])
    run:
        #shell("module unload gcc && module load tools perl/5.20.2 metabat/2.12.1 && jgi_summarize_bam_contig_depths --outputDepth {output.depth_file} {input.assemblybam}")
        shell("module unload gcc && module load tools perl/5.20.2 metabat/2.12.1 && mkdir {output.dir_mtb} && metabat2 -i {input.assembly_idx} -a {input.depth_file} -o {output.dir_mtb} -m 1500 -t {params.threads} --unbinned")

    #Create contig to bin table

        bintable = open(str(output.bin_table_mtb),"a+")

        binlist=glob.glob(str(dir_mtb+"*"))

        # metabatdir = os.path.join(projectpath,"07-Binning")
        #binlist = glob.glob(metabatdir)
        for bin in binlist:
            binname = os.path.splitext(os.path.basename(bin))[0]+''
            with open(bin, 'r') as binfile:
               for line in binfile:
                    if line.startswith('>'):
                        contig = line.strip()
                        contig = contig.replace(">", "")
                        bintable.write("{0}\t{1}\r\n".format(contig,binname))
        bintable.close()

        shell("tar -czvf {output.final_file} {output.dir_mtb}*.fa")
##
# Binning with maxbin
##


rule binning_maxbin:
    input:
        assembly_idx="{projectpath}/05-Assembly/{sample}/{sample}.fa",
        #assemblybam="{projectpath}/06-Assembly_mapping/{sample}.mapped.bam"
        depth_file="{projectpath}/07-Binning/{sample}.depth.txt"
    output:
        dir_mxb="{projectpath}/07-Binning/{sample}.maxbin",
        #depth_file="{projectpath}/07-Binning/{sample}.depth_maxbin.txt",
        bin_table_mxb="{projectpath}/07-Binning/{sample}.bins_maxbin.txt",
        final_file="{projectpath}/07-Binning/{sample}.bins_maxbin.tar.gz"
    params:
        threads=expand("{threads}", threads=config['threads'])
    run:
        #shell("module unload gcc && module load tools perl/5.20.2 metabat/2.12.1 && jgi_summarize_bam_contig_depths --outputDepth {output.depth_file}--noIntraDepthVariance {input.assemblybam}")
        shell("module unload gcc && module load tools perl/5.20.2 maxbin/2.2.7 fraggenescan/1.31 && mkdir {output.dir_mxb} && run_MaxBin.pl -contig {input.assembly_idx} -abund {input.depth_file}* -out {output.dir_mxb} -thread {params.threads}")

        #Generate bin table
        bintable = open(str(output.bin_table_mxb),"a+")

        binlist=glob.glob(str(dir_mxb+"*"))

        #maxbindir = os.path.join(output.dir_mxb + 'bin*fa*')
        #binlist = glob.glob(maxbindir)
        for bin in binlist:
            binname = os.path.splitext(os.path.basename(bin))[0]+''
            with open(bin, 'r') as binfile:
               for line in binfile:
                    if line.startswith('>'):
                        contig = line.strip()
                        contig = contig.replace(">", "")
                        bintable.write("{0}\t{1}\r\n".format(contig,binname))
        bintable.close()

        shell("tar -czvf {output.final_file} {output.dir_mxb}*.fasta")

##
# Bin refinement with DASTool using binning: metabat, maxbin and proteins from: prodigal
##
 # --proteins                 Predicted proteins in prodigal fasta format (>scaffoldID_geneNo).
 #                              Gene prediction step will be skipped if given. (optional)

rule bin_refinement:
    input:
        assembly_idx="{projectpath}/05-Assembly/{sample}/{sample}.fa",
        metabat_bintable="{projectpath}/07-Binning/{sample}.bins_metabat.txt",
        maxbin_bintable="{projectpath}/07-Binning/{sample}.bins_maxbin.txt*",
        pproteins="{projectpath}/06-ProdigalPrediction/{sample}.protein_translations.faa"
    output:
        main_dir=directory("{projectpath}/07-Binning/{sample}_BinRefinement"),
        bin_dir=directory("{projectpath}/07-Binning/{sample}_Dastool_bins")
    params:
        threads=expand("{threads}", threads=config['threads']),
        dastoolDependencies=expand("{dastoolDependencies}", dastoolDependencies=config['dastoolDependencies']),
        search_eng=expand("{search_eng}", search_eng=config['search_eng']),
        dastool_db=expand("{dastool_db}", dastool_db=config['dastool_db'])
    run:
        bincontig_tables=",".join(glob.glob({input.metabat_bintable},{input.maxbin_bintable}))
        shell("{params.dastoolDependencies} && DAS_Tool -i bincontig_tables -c {input.assembly_idx} -o {output.main_dir} --proteins {input.pproteins} -l maxbin,metabat --search_engine {params.search_eng} -t {params.threads} --db_directory {params.dastool_db} --write_bins 1")

        #Move definitive bins to a new directory /Dastool_bins
        import os
        import glob
        binsource=output.main_dir
        binfiles = glob.glob(os.path.join(binsource,'*.fa'))
        for b in binfiles:
            shutil.move(b, output.bin_dir)




print("############################ Holoflow has finished the METAGENOMICS workflow :) ############################")
